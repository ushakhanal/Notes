## Recon Badge

### Recon 00 
```
The robots.txt file is used to tell web spiders how to crawl a website.This is done using the keyword Disallow.
```

### Recon 01
```
Not Found/404 pages can leak information about the web stack used by a company or application.
It also allows you to detect files that exists when you start bruteforcing directory.
This is why it is important to check what the 404 page looks like.
```

### Recon 02 
```
The security.txt file is used to tell security researchers how they can disclose vulnerabilities for a website
```

### Recon 03 
```
An "index" file is present and it will get returned. N.B.: the file is not necessarily named index, this can be configured.
But most of the time, the file will be named index.html.
To find directories, with indexing turned on. You need to browse the source of the HTML pages and look at the directories used to store files. 
Once you have a list of directories, you can access each of them individually
```

### Recon 04 
```
When accessing a new webserver, it often pays off to manually check for some directories before starting to brute force using a tool. 
For example, you can manually check for /admin/.
```

### Recon 05 
```
When accessing a new webserver, it often pays off to brute force directories.
To do this, you can use many tools like patator, FFUF or WFuzz (amongst many others).
```

### Recon 06 
```
When accessing a new webserver, it often pays off to replace the hostname with the IP address or to provide a random Host header in the request. 
To do this, you can either modify the request in a web proxy or use curl -H "Host: ....".
```

### Recon 07 
```
When accessing a new webserver, it often pays off to replace the hostname with the IP address or to provide a random Host header in the request.
To do this, you can either modify the request in a web proxy or use curl -H "Host: ....".
This time you need to check the TLS version of the website to get the key
```

### Recon 08 
```
Looking for alternative names can be done in your client or by using openssl.
It's common for TLS servers to have certificates that are valid for more than one name (named alternative names).
```

### Recon 09 
```
When accessing a web server, it often pays off to check the responses' headers.
It's common to find information around version and technologies used.
```

### Recon 10 
```
If you haven't done visual reconnaissance before, 
you can try to use the tool Aquatone to get images that you can browse easily to find the right key.
We can also use tools like "*Eyewitness*"
```

### Recon 11 
```
The robots.txt file is used to tell web spiders how to crawl a website.
This is done using the keyword Disallow.
```

### Recon 12 
```
Serving requests for a single application can be done by multiple backends.
It can pay off to send the same request multiple times to check if multiple backends are involved.
```

### Recon 13 
```
TXT records are often used to show that people own a domain or to store information to configure services, 
it's always a good idea to check for those.
```

### Recon 14
```
The robots.txt file is used to tell web spiders how to crawl a website.This is done using the keyword Disallow.
```
### Recon 15
### Recon 16
```
Bind is one of the most common DNS server used. If you know how to ask, it will reveal you its version.
```
### Recon 17
### Recon 18
### Recon 19
### Recon 20
### Recon 21
### Recon 22
### Recon 23
### Recon 24
